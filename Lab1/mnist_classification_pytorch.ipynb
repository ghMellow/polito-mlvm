{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iBLIGYhsQPS"
      },
      "source": [
        "# MNIST Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyHNIz6QsURr"
      },
      "source": [
        "**Import packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0xm3t7q504v8"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb5ErZdMsfzX"
      },
      "source": [
        "## Define a fully connected Neural Network\n",
        "We define a very simple fully connected neural network. It is a subclass of torch.nn.Module. <br>\n",
        "When extending torch.nn.Module we need to define two methods:\n",
        "- `__init__` method:\n",
        "it is the constructor of the class and is called only once, when instantiating the neural network. It initializes the network's architecture by initializing all network layers and other components. It is where you define the structure of the network, such as layers, activation functions, and any other components required for computation.\n",
        "-`forward` method\n",
        "is called when you pass input data through the network. The forward method specifies the sequence of operations that are applied to the input data. It describes how the input data flow through the various layers to produce the final output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YXxGAM0H35lj"
      },
      "outputs": [],
      "source": [
        "# define neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.lin1 = nn.Linear(28*28, 512)\n",
        "        self.act1 = nn.ReLU()\n",
        "\n",
        "        self.lin2 = nn.Linear(512, 512)\n",
        "        self.act2 = nn.ReLU()\n",
        "\n",
        "        self.output_layer = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)  # (batch_size, 28, 28) => (batch_size, 28*28)\n",
        "\n",
        "        # first layer (input is x, output is x1)\n",
        "        x1 = self.lin1(x)\n",
        "        x1 = self.act1(x1)\n",
        "\n",
        "        # second layer (input is x1, output is x2)\n",
        "        x2 = self.lin2(x1)\n",
        "        x2 = self.act2(x2)\n",
        "\n",
        "        # third/output layer (input is x2, output is logits)\n",
        "        logits = self.output_layer(x2)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhJqY-ZJQaHO"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1bZT80E4gdY",
        "outputId": "ca95e117-f8c5-4ed8-d78b-fd5e210fed23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File nella cartella raw:\n",
            "  - t10k-images-idx3-ubyte.gz\n",
            "  - train-images-idx3-ubyte.gz\n",
            "  - train-labels-idx1-ubyte.gz\n",
            "  - t10k-labels-idx1-ubyte.gz\n",
            "Errore: Dataset not found. You can use download=True to download it\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "\n",
        "# Verifica che i file esistano\n",
        "raw_folder = './data/MNIST/raw'\n",
        "print(\"File nella cartella raw:\")\n",
        "for file in os.listdir(raw_folder):\n",
        "    print(f\"  - {file}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
        "])\n",
        "\n",
        "# Prova a caricare\n",
        "try:\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=False, transform=transform)\n",
        "    print(f\"✓ MNIST train dataset - num samples: {len(train_dataset)}\")\n",
        "    \n",
        "    test_dataset = datasets.MNIST('./data', train=False, download=False, transform=transform)\n",
        "    print(f\"✓ MNIST test dataset - num samples: {len(test_dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Errore: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ MNIST train dataset - num samples: 60000\n",
            "✓ MNIST test dataset - num samples: 10000\n",
            "✓ Image shape: torch.Size([1, 28, 28])\n",
            "✓ Label: 5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "class CustomMNIST(Dataset):\n",
        "    def __init__(self, images_path, labels_path, transform=None):\n",
        "        # Carica immagini\n",
        "        with gzip.open(images_path, 'rb') as f:\n",
        "            self.images = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28)\n",
        "        \n",
        "        # Carica labels\n",
        "        with gzip.open(labels_path, 'rb') as f:\n",
        "            self.labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
        "        \n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx].astype('float32') / 255.0\n",
        "        label = self.labels[idx]\n",
        "        \n",
        "        # Converti in tensor e aggiungi dimensione canale\n",
        "        image = torch.tensor(image).unsqueeze(0)  # (28, 28) -> (1, 28, 28)\n",
        "        \n",
        "        # Applica la normalizzazione\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "# Usa il dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "# Solo normalizzazione (ToTensor già fatto nel __getitem__)\n",
        "transform = transforms.Normalize((0.1307,), (0.3081,))\n",
        "\n",
        "train_dataset = CustomMNIST(\n",
        "    './data/MNIST/raw/train-images-idx3-ubyte.gz',\n",
        "    './data/MNIST/raw/train-labels-idx1-ubyte.gz',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = CustomMNIST(\n",
        "    './data/MNIST/raw/t10k-images-idx3-ubyte.gz',\n",
        "    './data/MNIST/raw/t10k-labels-idx1-ubyte.gz',\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print(f\"✓ MNIST train dataset - num samples: {len(train_dataset)}\")\n",
        "print(f\"✓ MNIST test dataset - num samples: {len(test_dataset)}\")\n",
        "\n",
        "# Test: carica un'immagine\n",
        "image, label = train_dataset[0]\n",
        "print(f\"✓ Image shape: {image.shape}\")  # Dovrebbe essere torch.Size([1, 28, 28])\n",
        "print(f\"✓ Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHPV6AlpCemt"
      },
      "source": [
        "### Plot one sample from dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-plmwVVLH6li"
      },
      "outputs": [],
      "source": [
        "image, label = train_dataset[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qct8Yib_IKo6",
        "outputId": "13fd26c7-9c5f-48d1-f2c0-0a48bdce8f89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHIdA_3oITeR",
        "outputId": "175aa773-53fc-46aa-957c-ac5809928756"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.uint8(5)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWOVzrKN56Id",
        "outputId": "f01025e8-c9ad-47b5-ce36-a63ec4cf71ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'tuple'>\n",
            "2\n",
            "image:  torch.Size([1, 28, 28]) <class 'torch.Tensor'>\n",
            "label:  5 <class 'numpy.uint8'>\n"
          ]
        }
      ],
      "source": [
        "# dataset returns one sample at a time\n",
        "# In our case one sample corresponds to a Tuple: (Image, Target Label)\n",
        "# Image is a tensor with shape [1, 28, 28]\n",
        "# Label is an integer representing the class of the image\n",
        "\n",
        "print(type(train_dataset[100]))\n",
        "print(len(train_dataset[100]))\n",
        "\n",
        "image, label = train_dataset[100]\n",
        "print(\"image: \", image.shape, type(image))\n",
        "print(\"label: \", label, type(label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VglQ4o-OI2PM",
        "outputId": "ea3a617b-337b-45ea-fbaf-6f36661c56dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eYKl3xNI5NH",
        "outputId": "30bb7cbd-607a-419a-e1dd-6323618739b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1eqKWkgI9GR",
        "outputId": "c1a071d4-d2e6-4db2-d6b3-7ac8cd936996"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image = image.view(28,28)\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "_JAtWFQkR_0Y",
        "outputId": "f1da70ee-c0b9-4ae5-ee5c-96b70421da69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x12f8c6690>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot one sample normalized\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(image.numpy())  # numpy array 28x28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSqvt0wTYB9C"
      },
      "source": [
        "## Dataloader\n",
        "Dataloader reads from the dataset and returns batches of data. We need to specify at least:\n",
        "- `dataset`: torch.utils.data.Dataset object from which the DataLoader fetch data\n",
        "- `batch_size`: number of sample in a batch, e.g. 128, choose the maximum that the GPU can fit\n",
        "\n",
        "Other parameters are:\n",
        "- `shuffle`: boolean, whether we want to shuffle the data samples\n",
        "- `drop_last`: boolean, whether the last batch, which may have fewer samples than the specified batch size, should be included or dropped\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rjxpi3J42oR",
        "outputId": "a2714e21-c703-488f-f957-4a43aa0e8729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch size: 128\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128  # we define here the batch size: number of samples processed before the model is updated\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True) #, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False) # , drop_last=False)\n",
        "print(f\"batch size: {batch_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP8VtriNSMFT"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "3HlkzQx64GZW"
      },
      "outputs": [],
      "source": [
        "# function for training\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()  # model to train mode\n",
        "\n",
        "    # ITERATE DATALOADER: train_loader\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #  SINGLE OPTIMIZATION STEP IS PERFORMED ON A BATCH!\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "\n",
        "        loss = F.cross_entropy(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "# test\n",
        "# function for evaluation\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()  # model to eval\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # ITERATE DATALOADER: test_loader\n",
        "    for data, target in test_loader:\n",
        "        batch_size = data.shape[0]\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "\n",
        "        # sanity check\n",
        "        pred = pred.view(batch_size)  # [bs,]\n",
        "        target = target.view(batch_size)  # [bs,]\n",
        "\n",
        "        # compute prediction ok\n",
        "        batch_pred_ok = pred.eq(target).sum().item()\n",
        "        correct += batch_pred_ok\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    num_samples = len(test_loader.dataset)\n",
        "    test_accuracy = correct / num_samples\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D609MxPV7gvZ",
        "outputId": "8a71e5e8-6063-4ff2-9176-db63e10ffc4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Using MPS (Metal Performance Shaders) - Apple GPU\n",
            "lr: 0.01\n",
            "batch size: 128\n",
            "Num. optimization steps per-epoch: 468\n"
          ]
        }
      ],
      "source": [
        "# Usa MPS per Mac (Apple Silicon) o CPU come fallback\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print(\"✓ Using MPS (Metal Performance Shaders) - Apple GPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"✓ Using CUDA - NVIDIA GPU\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"✓ Using CPU\")\n",
        "\n",
        "# training hyperparameters\n",
        "lr = 0.01\n",
        "num_epochs = 5\n",
        "print(f\"lr: {lr}\")\n",
        "print(f\"batch size: {batch_size}\")\n",
        "print(f\"Num. optimization steps per-epoch: {int(len(train_dataset)/batch_size)}\")\n",
        "\n",
        "#########\n",
        "# MODEL #\n",
        "#########\n",
        "model = NeuralNetwork(num_classes=10)\n",
        "model = model.to(device)\n",
        "\n",
        "#############\n",
        "# OPTIMIZER #\n",
        "#############\n",
        "parameters_to_optimize = model.parameters()\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=lr, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ7HhNC7S8Ny",
        "outputId": "e54b213f-fdfc-4004-a875-7d0f2540761e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.329612\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.765130\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.272953\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.402543\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.350140\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.350037\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.245429\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.242869\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.180852\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.171246\n",
            "\n",
            "Test set: Average loss: 0.1692, Accuracy: 9472/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.347149\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.093997\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.089008\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.070041\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.107576\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.204547\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.163901\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.161367\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.100613\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.065865\n",
            "\n",
            "Test set: Average loss: 0.1173, Accuracy: 9626/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.200483\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.057833\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.102903\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.154479\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.044610\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.152547\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.138822\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.123320\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.082225\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.058636\n",
            "\n",
            "Test set: Average loss: 0.0852, Accuracy: 9725/10000 (97%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.036378\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.036524\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.100295\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.094440\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.090862\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.039347\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.063899\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.037025\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.044968\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.061187\n",
            "\n",
            "Test set: Average loss: 0.0756, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.024958\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.015621\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.041615\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.060896\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.018204\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.041787\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.080256\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.038417\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.037995\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.047198\n",
            "\n",
            "Test set: Average loss: 0.0718, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "CPU times: user 11.6 s, sys: 1.33 s, total: 12.9 s\n",
            "Wall time: 17.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  train(model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lab1-L4vtdFOp-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
